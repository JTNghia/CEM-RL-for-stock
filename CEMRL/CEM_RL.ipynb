{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CEM_RL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uMcs-xjf4MQD",
        "gAdJybWi4-tm",
        "mWmjCZdi5JdE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr6-REecIZjc",
        "outputId": "b5698c29-723c-46c0-d645-717a3cb2a528"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0L4reKKD04Oj"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "import gym\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random process"
      ],
      "metadata": {
        "id": "R62Kcx5b4FMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OrnsteinUhlenbeckProcess:\n",
        "    \"\"\"\n",
        "    Ornstein-Uhnlenbeck process\n",
        "    Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):\n",
        "        self.action_dim = action_dim\n",
        "        self.mu = mu\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.X = np.ones(self.action_dim) * self.mu\n",
        "\n",
        "    def reset(self):\n",
        "        self.X = np.ones(self.action_dim) * self.mu\n",
        "\n",
        "    def sample(self):\n",
        "        dx = self.theta * (self.mu - self.X)\n",
        "        dx = dx + self.sigma * np.random.randn(len(self.X))\n",
        "        self.X = self.X + dx\n",
        "        return self.X\n"
      ],
      "metadata": {
        "id": "_DTqDoOh1K0t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNoise:\n",
        "    \"\"\"\n",
        "    Simple Gaussian noise\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_dim, sigma=0.2):\n",
        "        self.action_dim = action_dim\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def sample(self):\n",
        "        s = np.random.normal(scale=self.sigma, size=self.action_dim)\n",
        "        return s"
      ],
      "metadata": {
        "id": "Qa17etxn18Eh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "uMcs-xjf4MQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RLNN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(RLNN, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def set_params(self, params):\n",
        "        \"\"\"\n",
        "        Set the params of the network to the given parameters\n",
        "        \"\"\"\n",
        "        cpt = 0\n",
        "        for param in self.parameters():\n",
        "            tmp = np.product(param.size())\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                param.data.copy_(torch.from_numpy(\n",
        "                    params[cpt:cpt + tmp]).view(param.size()).cuda())\n",
        "            else:\n",
        "                param.data.copy_(torch.from_numpy(\n",
        "                    params[cpt:cpt + tmp]).view(param.size()))\n",
        "            cpt += tmp\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Returns parameters of the actor\n",
        "        \"\"\"\n",
        "        return deepcopy(np.hstack([to_numpy(v).flatten() for v in\n",
        "                                   self.parameters()]))\n",
        "\n",
        "    def get_grads(self):\n",
        "        \"\"\"\n",
        "        Returns the current gradient\n",
        "        \"\"\"\n",
        "        return deepcopy(np.hstack([to_numpy(v.grad).flatten() for v in self.parameters()]))\n",
        "\n",
        "    def get_size(self):\n",
        "        \"\"\"\n",
        "        Returns the number of parameters of the network\n",
        "        \"\"\"\n",
        "        return self.get_params().shape[0]\n",
        "\n",
        "    def load_model(self, filename, net_name):\n",
        "        \"\"\"\n",
        "        Loads the model\n",
        "        \"\"\"\n",
        "        if filename is None:\n",
        "            return\n",
        "\n",
        "        self.load_state_dict(\n",
        "            torch.load('{}/{}.pkl'.format(filename, net_name),\n",
        "                       map_location=lambda storage, loc: storage)\n",
        "        )\n",
        "\n",
        "    def save_model(self, output, net_name):\n",
        "        \"\"\"\n",
        "        Saves the model\n",
        "        \"\"\"\n",
        "        torch.save(\n",
        "            self.state_dict(),\n",
        "            '{}/{}.pkl'.format(output, net_name)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "l7tyZORC1_fU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ES"
      ],
      "metadata": {
        "id": "7w1CNzNB4RHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sepCEM:\n",
        "\n",
        "    \"\"\"\n",
        "    Cross-entropy methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_params,\n",
        "                 mu_init=None,\n",
        "                 sigma_init=1e-3,\n",
        "                 pop_size=256,\n",
        "                 damp=1e-3,\n",
        "                 damp_limit=1e-5,\n",
        "                 parents=None,\n",
        "                 elitism=False,\n",
        "                 antithetic=False):\n",
        "\n",
        "        # misc\n",
        "        self.num_params = num_params\n",
        "\n",
        "        # distribution parameters\n",
        "        if mu_init is None:\n",
        "            self.mu = np.zeros(self.num_params)\n",
        "        else:\n",
        "            self.mu = np.array(mu_init)\n",
        "        self.sigma = sigma_init\n",
        "        self.damp = damp\n",
        "        self.damp_limit = damp_limit\n",
        "        self.tau = 0.95\n",
        "        self.cov = self.sigma * np.ones(self.num_params)\n",
        "\n",
        "        # elite stuff\n",
        "        self.elitism = elitism\n",
        "        self.elite = np.sqrt(self.sigma) * np.random.rand(self.num_params)\n",
        "        self.elite_score = None\n",
        "\n",
        "        # sampling stuff\n",
        "        self.pop_size = pop_size\n",
        "        self.antithetic = antithetic\n",
        "\n",
        "        if self.antithetic:\n",
        "            assert (self.pop_size % 2 == 0), \"Population size must be even\"\n",
        "        if parents is None or parents <= 0:\n",
        "            self.parents = pop_size // 2\n",
        "        else:\n",
        "            self.parents = parents\n",
        "        self.weights = np.array([np.log((self.parents + 1) / i)\n",
        "                                 for i in range(1, self.parents + 1)])\n",
        "        self.weights /= self.weights.sum()\n",
        "\n",
        "    def ask(self, pop_size):\n",
        "        \"\"\"\n",
        "        Returns a list of candidates parameters\n",
        "        \"\"\"\n",
        "        if self.antithetic and not pop_size % 2:\n",
        "            epsilon_half = np.random.randn(pop_size // 2, self.num_params)\n",
        "            epsilon = np.concatenate([epsilon_half, - epsilon_half])\n",
        "\n",
        "        else:\n",
        "            epsilon = np.random.randn(pop_size, self.num_params)\n",
        "\n",
        "        inds = self.mu + epsilon * np.sqrt(self.cov)\n",
        "        if self.elitism:\n",
        "            inds[-1] = self.elite\n",
        "\n",
        "        return inds\n",
        "\n",
        "    def tell(self, solutions, scores):\n",
        "        \"\"\"\n",
        "        Updates the distribution\n",
        "        \"\"\"\n",
        "        scores = np.array(scores)\n",
        "        scores *= -1\n",
        "        idx_sorted = np.argsort(scores)\n",
        "\n",
        "        old_mu = self.mu\n",
        "        self.damp = self.damp * self.tau + (1 - self.tau) * self.damp_limit\n",
        "        self.mu = self.weights @ solutions[idx_sorted[:self.parents]]\n",
        "\n",
        "        z = (solutions[idx_sorted[:self.parents]] - old_mu)\n",
        "        self.cov = 1 / self.parents * self.weights @ (\n",
        "            z * z) + self.damp * np.ones(self.num_params)\n",
        "\n",
        "        self.elite = solutions[idx_sorted[0]]\n",
        "        self.elite_score = scores[idx_sorted[0]]\n",
        "        print(self.cov)\n",
        "\n",
        "    def get_distrib_params(self):\n",
        "        \"\"\"\n",
        "        Returns the parameters of the distrubtion:\n",
        "        the mean and sigma\n",
        "        \"\"\"\n",
        "        return np.copy(self.mu), np.copy(self.cov)\n",
        "\n",
        "\n",
        "class Control:\n",
        "\n",
        "    \"\"\"\n",
        "    Cross-entropy methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_params, mu_init, pop_size=256, sigma_init=1e-3):\n",
        "\n",
        "        # misc\n",
        "        self.num_params = num_params\n",
        "        self.pop = np.sqrt(sigma_init) * np.random.randn(pop_size, num_params) + mu_init\n",
        "        self.mu = np.zeros(num_params)\n",
        "\n",
        "    def ask(self, pop_size):\n",
        "        \"\"\"\n",
        "        Returns a list of candidates parameters\n",
        "        \"\"\"\n",
        "        return self.pop\n",
        "\n",
        "    def tell(self, solutions, scores):\n",
        "        \"\"\"\n",
        "        Updates the distribution\n",
        "        \"\"\"\n",
        "        self.mu = solutions[np.argmax(scores)]\n",
        "        self.pop = solutions\n",
        "        np.random.shuffle(self.pop)\n"
      ],
      "metadata": {
        "id": "paVSU4Go2Ve5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory"
      ],
      "metadata": {
        "id": "gAdJybWi4-tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Memory():\n",
        "\n",
        "    def __init__(self, memory_size, state_dim, action_dim):\n",
        "\n",
        "        # params\n",
        "        self.memory_size = memory_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "\n",
        "        if USE_CUDA:\n",
        "            self.states = torch.zeros(self.memory_size, self.state_dim).cuda()\n",
        "            self.actions = torch.zeros(\n",
        "                self.memory_size, self.action_dim).cuda()\n",
        "            self.n_states = torch.zeros(\n",
        "                self.memory_size, self.state_dim).cuda()\n",
        "            self.rewards = torch.zeros(self.memory_size, 1).cuda()\n",
        "            self.dones = torch.zeros(self.memory_size, 1).cuda()\n",
        "\n",
        "        else:\n",
        "            self.states = torch.zeros(self.memory_size, self.state_dim)\n",
        "            self.actions = torch.zeros(self.memory_size, self.action_dim)\n",
        "            self.n_states = torch.zeros(self.memory_size, self.state_dim)\n",
        "            self.rewards = torch.zeros(self.memory_size, 1)\n",
        "            self.dones = torch.zeros(self.memory_size, 1)\n",
        "\n",
        "    def size(self):\n",
        "        if self.full:\n",
        "            return self.memory_size\n",
        "        return self.pos\n",
        "\n",
        "    def get_pos(self):\n",
        "        return self.pos\n",
        "\n",
        "    # Expects tuples of (state, next_state, action, reward, done)\n",
        "\n",
        "    def add(self, datum):\n",
        "\n",
        "        state, n_state, action, reward, done = datum\n",
        "\n",
        "        self.states[self.pos] = FloatTensor(state)\n",
        "        self.n_states[self.pos] = FloatTensor(n_state)\n",
        "        self.actions[self.pos] = FloatTensor(action)\n",
        "        self.rewards[self.pos] = FloatTensor([reward])\n",
        "        self.dones[self.pos] = FloatTensor([done])\n",
        "\n",
        "        self.pos += 1\n",
        "        if self.pos == self.memory_size:\n",
        "            self.full = True\n",
        "            self.pos = 0\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "\n",
        "        upper_bound = self.memory_size if self.full else self.pos\n",
        "        batch_inds = torch.LongTensor(\n",
        "            np.random.randint(0, upper_bound, size=batch_size))\n",
        "\n",
        "        return (self.states[batch_inds],\n",
        "                self.n_states[batch_inds],\n",
        "                self.actions[batch_inds],\n",
        "                self.rewards[batch_inds],\n",
        "                self.dones[batch_inds])\n",
        "\n",
        "    def get_reward(self, start_pos, end_pos):\n",
        "\n",
        "        tmp = 0\n",
        "        if start_pos <= end_pos:\n",
        "            for i in range(start_pos, end_pos):\n",
        "                tmp += self.rewards[i]\n",
        "        else:\n",
        "            for i in range(start_pos, self.memory_size):\n",
        "                tmp += self.rewards[i]\n",
        "\n",
        "            for i in range(end_pos):\n",
        "                tmp += self.rewards[i]\n",
        "\n",
        "        return tmp\n",
        "\n",
        "    def repeat(self, start_pos, end_pos):\n",
        "\n",
        "        if start_pos <= end_pos:\n",
        "            for i in range(start_pos, end_pos):\n",
        "\n",
        "                self.states[self.pos] = self.states[i].clone()\n",
        "                self.n_states[self.pos] = self.n_states[i].clone()\n",
        "                self.actions[self.pos] = self.actions[i].clone()\n",
        "                self.rewards[self.pos] = self.rewards[i].clone()\n",
        "                self.dones[self.pos] = self.dones[i].clone()\n",
        "\n",
        "                self.pos += 1\n",
        "                if self.pos == self.memory_size:\n",
        "                    self.full = True\n",
        "                    self.pos = 0\n",
        "\n",
        "        else:\n",
        "            for i in range(start_pos, self.memory_size):\n",
        "\n",
        "                self.states[self.pos] = self.states[i].clone()\n",
        "                self.n_states[self.pos] = self.n_states[i].clone()\n",
        "                self.actions[self.pos] = self.actions[i].clone()\n",
        "                self.rewards[self.pos] = self.rewards[i].clone()\n",
        "                self.dones[self.pos] = self.dones[i].clone()\n",
        "\n",
        "                self.pos += 1\n",
        "                if self.pos == self.memory_size:\n",
        "                    self.full = True\n",
        "                    self.pos = 0\n",
        "\n",
        "            for i in range(end_pos):\n",
        "\n",
        "                self.states[self.pos] = self.states[i].clone()\n",
        "                self.n_states[self.pos] = self.n_states[i].clone()\n",
        "                self.actions[self.pos] = self.actions[i].clone()\n",
        "                self.rewards[self.pos] = self.rewards[i].clone()\n",
        "                self.dones[self.pos] = self.dones[i].clone()\n",
        "\n",
        "                self.pos += 1\n",
        "                if self.pos == self.memory_size:\n",
        "                    self.full = True\n",
        "                    self.pos = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "mOd2xr0Q458J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parameters"
      ],
      "metadata": {
        "id": "eUlpCwyvwd3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--mode', default='train', type=str,)\n",
        "parser.add_argument('--env', default='HalfCheetah-v2', type=str)\n",
        "parser.add_argument('--start_steps', default=10000, type=int)\n",
        "\n",
        "# DDPG parameters\n",
        "parser.add_argument('--actor_lr', default=0.001, type=float)\n",
        "parser.add_argument('--critic_lr', default=0.001, type=float)\n",
        "parser.add_argument('--batch_size', default=100, type=int)\n",
        "parser.add_argument('--discount', default=0.99, type=float)\n",
        "parser.add_argument('--reward_scale', default=1., type=float)\n",
        "parser.add_argument('--tau', default=0.005, type=float)\n",
        "parser.add_argument('--layer_norm', dest='layer_norm', action='store_true')\n",
        "\n",
        "# TD3 parameters\n",
        "parser.add_argument('--use_td3', dest='use_td3', action='store_true')\n",
        "parser.add_argument('--policy_noise', default=0.2, type=float)\n",
        "parser.add_argument('--noise_clip', default=0.5, type=float)\n",
        "parser.add_argument('--policy_freq', default=2, type=int)\n",
        "\n",
        "# Gaussian noise parameters\n",
        "parser.add_argument('--gauss_sigma', default=0.1, type=float)\n",
        "\n",
        "# OU process parameters\n",
        "parser.add_argument('--ou_noise', dest='ou_noise', action='store_true')\n",
        "parser.add_argument('--ou_theta', default=0.15, type=float)\n",
        "parser.add_argument('--ou_sigma', default=0.2, type=float)\n",
        "parser.add_argument('--ou_mu', default=0.0, type=float)\n",
        "\n",
        "# ES parameters\n",
        "parser.add_argument('--pop_size', default=10, type=int)\n",
        "parser.add_argument('--elitism', dest=\"elitism\",  action='store_true')\n",
        "parser.add_argument('--n_grad', default=5, type=int)\n",
        "parser.add_argument('--sigma_init', default=1e-3, type=float)\n",
        "parser.add_argument('--damp', default=1e-3, type=float)\n",
        "parser.add_argument('--damp_limit', default=1e-5, type=float)\n",
        "parser.add_argument('--mult_noise', dest='mult_noise', action='store_true')\n",
        "\n",
        "# Training parameters\n",
        "parser.add_argument('--n_episodes', default=1, type=int)\n",
        "parser.add_argument('--max_steps', default=1000000, type=int)\n",
        "parser.add_argument('--mem_size', default=1000000, type=int)\n",
        "parser.add_argument('--n_noisy', default=0, type=int)\n",
        "\n",
        "# Testing parameters\n",
        "parser.add_argument('--filename', default=\"\", type=str)\n",
        "parser.add_argument('--n_test', default=1, type=int)\n",
        "\n",
        "# misc\n",
        "parser.add_argument('--output', default='/content/results/', type=str)\n",
        "parser.add_argument('--period', default=5000, type=int)\n",
        "parser.add_argument('--n_eval', default=10, type=int)\n",
        "parser.add_argument('--save_all_models',\n",
        "                    dest=\"save_all_models\", action=\"store_true\")\n",
        "parser.add_argument('--debug', dest='debug', action='store_true')\n",
        "parser.add_argument('--seed', default=-1, type=int)\n",
        "parser.add_argument('--render', dest='render', action='store_true')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRihZXnJwiqu",
        "outputId": "82672a71-6a7d-4676-fd91-838afa77544b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--render'], dest='render', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def prRed(prt):\n",
        "    print(\"\\033[91m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prGreen(prt):\n",
        "    print(\"\\033[92m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prYellow(prt):\n",
        "    print(\"\\033[93m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prLightPurple(prt):\n",
        "    print(\"\\033[94m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prPurple(prt):\n",
        "    print(\"\\033[95m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prCyan(prt):\n",
        "    print(\"\\033[96m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prLightGray(prt):\n",
        "    print(\"\\033[97m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def prBlack(prt):\n",
        "    print(\"\\033[98m{}\\033[00m\" .format(prt))\n",
        "\n",
        "\n",
        "def to_numpy(var):\n",
        "    return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()\n",
        "\n",
        "\n",
        "def to_tensor(x, dtype=\"float\"):\n",
        "    \"\"\"\n",
        "    Numpy array to tensor\n",
        "    \"\"\"\n",
        "\n",
        "    FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
        "    ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n",
        "\n",
        "    if dtype == \"float\":\n",
        "        x = np.array(x, dtype=np.float64).tolist()\n",
        "        return FloatTensor(x)\n",
        "    elif dtype == \"long\":\n",
        "        x = np.array(x, dtype=np.long).tolist()\n",
        "        return LongTensor(x)\n",
        "    elif dtype == \"byte\":\n",
        "        x = np.array(x, dtype=np.byte).tolist()\n",
        "        return ByteTensor(x)\n",
        "    else:\n",
        "        x = np.array(x, dtype=np.float64).tolist()\n",
        "\n",
        "    return FloatTensor(x)\n",
        "\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "    \"\"\"\n",
        "    Performs a soft target update\n",
        "    \"\"\"\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            target_param.data * (1.0 - tau) + param.data * tau\n",
        "        )\n",
        "\n",
        "\n",
        "def hard_update(target, source):\n",
        "    \"\"\"\n",
        "    Performs a hard target update\n",
        "    \"\"\"\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(param.data)\n",
        "\n",
        "def get_output_folder(parent_dir, env_name):\n",
        "    \"\"\"Return save folder.\n",
        "    Assumes folders in the parent_dir have suffix -run{run\n",
        "    number}. Finds the highest run number and sets the output folder\n",
        "    to that number + 1. This is just convenient so that if you run the\n",
        "    same script multiple times tensorboard can plot all of the results\n",
        "    on the same plots with different names.\n",
        "    Parameters\n",
        "    ----------\n",
        "    parent_dir: str\n",
        "      Path of the directory containing all experiment runs.\n",
        "    Returns\n",
        "    -------\n",
        "    parent_dir/run_dir\n",
        "      Path to this run's save directory.\n",
        "    \"\"\"\n",
        "    os.makedirs(parent_dir, exist_ok=True)\n",
        "    experiment_id = 0\n",
        "    for folder_name in os.listdir(parent_dir):\n",
        "        if not os.path.isdir(os.path.join(parent_dir, folder_name)):\n",
        "            continue\n",
        "        try:\n",
        "            folder_name = int(folder_name.split('-run')[-1])\n",
        "            if folder_name > experiment_id:\n",
        "                experiment_id = folder_name\n",
        "        except:\n",
        "            pass\n",
        "    experiment_id += 1\n",
        "\n",
        "    parent_dir = os.path.join(parent_dir, env_name)\n",
        "    parent_dir = parent_dir + '-run{}'.format(experiment_id)\n",
        "    os.makedirs(parent_dir, exist_ok=True)\n",
        "    return parent_dir"
      ],
      "metadata": {
        "id": "1ZqzNXQ0xrDS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args()\n",
        "args.output = get_output_folder(args.output, args.env)\n",
        "with open(args.output + \"/parameters.txt\", 'w') as file:\n",
        "    for key, value in vars(args).items():\n",
        "        file.write(\"{} = {}\\n\".format(key, value))"
      ],
      "metadata": {
        "id": "z-IR5ij1xRmB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hope :)))"
      ],
      "metadata": {
        "id": "mWmjCZdi5JdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "if USE_CUDA:\n",
        "    FloatTensor = torch.cuda.FloatTensor\n",
        "else:\n",
        "    FloatTensor = torch.FloatTensor"
      ],
      "metadata": {
        "id": "5LsR7xb25K2K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(actor, env, memory=None, n_episodes=1, random=False, noise=None, render=False):\n",
        "    \"\"\"\n",
        "    Computes the score of an actor on a given number of runs,\n",
        "    fills the memory if needed\n",
        "    \"\"\"\n",
        "\n",
        "    if not random:\n",
        "        def policy(state):\n",
        "            state = FloatTensor(state.reshape(-1))\n",
        "            action = actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "            if noise is not None:\n",
        "                action += noise.sample()\n",
        "\n",
        "            return np.clip(action, -max_action, max_action)\n",
        "\n",
        "    else:\n",
        "        def policy(state):\n",
        "            return env.action_space.sample()\n",
        "\n",
        "    scores = []\n",
        "    steps = 0\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "\n",
        "        score = 0\n",
        "        obs = deepcopy(env.reset())\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # get next action and act\n",
        "            action = policy(obs)\n",
        "            n_obs, reward, done, _ = env.step(action)\n",
        "            # done_bool = 0 if steps + \\\n",
        "                # 1 == env._max_episode_steps else float(done)\n",
        "            score += reward\n",
        "            steps += 1\n",
        "\n",
        "            # adding in memory\n",
        "            if memory is not None:\n",
        "                memory.add((obs.flatten(), n_obs.flatten(), action, reward, done))\n",
        "            obs = n_obs\n",
        "\n",
        "            # render if needed\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            # reset when done\n",
        "            if done:\n",
        "                \n",
        "                env.reset()\n",
        "                break\n",
        "            \n",
        "            # print(f\"step {steps}, score: \",score)\n",
        "\n",
        "        scores.append(score)\n",
        "        \n",
        "    # print(\"MEAN SCORE: \",np.mean(scores))\n",
        "\n",
        "\n",
        "    return np.mean(scores), steps"
      ],
      "metadata": {
        "id": "bcLvQ9ES5LnU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(RLNN):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action, args):\n",
        "        super(Actor, self).__init__(state_dim, action_dim, max_action)\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        if args.layer_norm:\n",
        "            self.n1 = nn.LayerNorm(400)\n",
        "            self.n2 = nn.LayerNorm(300)\n",
        "        self.layer_norm = args.layer_norm\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.actor_lr)\n",
        "        self.tau = args.tau\n",
        "        self.discount = args.discount\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if not self.layer_norm:\n",
        "            x = torch.tanh(self.l1(x))\n",
        "            x = torch.tanh(self.l2(x))\n",
        "            x = self.max_action * torch.tanh(self.l3(x))\n",
        "\n",
        "        else:\n",
        "            x = torch.tanh(self.n1(self.l1(x)))\n",
        "            x = torch.tanh(self.n2(self.l2(x)))\n",
        "            x = self.max_action * torch.tanh(self.l3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def update(self, memory, batch_size, critic, actor_t):\n",
        "\n",
        "        # Sample replay buffer\n",
        "        states, _, _, _, _ = memory.sample(batch_size)\n",
        "\n",
        "        # Compute actor loss\n",
        "        if args.use_td3:\n",
        "            actor_loss = -critic(states, self(states))[0].mean()\n",
        "        else:\n",
        "            actor_loss = -critic(states, self(states)).mean()\n",
        "\n",
        "        # Optimize the actor\n",
        "        self.optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update the frozen target models\n",
        "        for param, target_param in zip(self.parameters(), actor_t.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            \n",
        "class Critic(RLNN):\n",
        "    def __init__(self, state_dim, action_dim, max_action, args):\n",
        "        super(Critic, self).__init__(state_dim, action_dim, 1)\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "        if args.layer_norm:\n",
        "            self.n1 = nn.LayerNorm(400)\n",
        "            self.n2 = nn.LayerNorm(300)\n",
        "\n",
        "        self.layer_norm = args.layer_norm\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.critic_lr)\n",
        "        self.tau = args.tau\n",
        "        self.discount = args.discount\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x, u):\n",
        "\n",
        "        if not self.layer_norm:\n",
        "            x = F.leaky_relu(self.l1(torch.cat([x, u], 1)))\n",
        "            x = F.leaky_relu(self.l2(x))\n",
        "            x = self.l3(x)\n",
        "\n",
        "        else:\n",
        "            x = F.leaky_relu(self.n1(self.l1(torch.cat([x, u], 1))))\n",
        "            x = F.leaky_relu(self.n2(self.l2(x)))\n",
        "            x = self.l3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def update(self, memory, batch_size, actor_t, critic_t):\n",
        "\n",
        "        # Sample replay buffer\n",
        "        states, n_states, actions, rewards, dones = memory.sample(batch_size)\n",
        "\n",
        "        # Q target = reward + discount * Q(next_state, pi(next_state))\n",
        "        with torch.no_grad():\n",
        "            target_Q = critic_t(n_states, actor_t(n_states))\n",
        "            target_Q = rewards + (1 - dones) * self.discount * target_Q\n",
        "\n",
        "        # Get current Q estimate\n",
        "        current_Q = self(states, actions)\n",
        "\n",
        "        # Compute critic loss\n",
        "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
        "\n",
        "        # Optimize the critic\n",
        "        self.optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update the frozen target models\n",
        "        for param, target_param in zip(self.parameters(), critic_t.parameters()):\n",
        "            target_param.data.copy_(\n",
        "                self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "metadata": {
        "id": "sjyqK18tGWug"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trading env"
      ],
      "metadata": {
        "id": "TaIrRkcpRhss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/UIT/Mạng Neural và Giải thuật di truyền/Project/CEMRL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r_cfAspImrb",
        "outputId": "70339dd3-86b5-4f2f-edd9-e0677b965f57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/UIT/Mạng Neural và Giải thuật di truyền/Project/CEMRL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/notadamking/Stock-Trading-Environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWxLrhqPhoBW",
        "outputId": "ff440cee-53d7-4992-e723-004f1271edda"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Stock-Trading-Environment' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/UIT/Mạng Neural và Giải thuật di truyền/Project/CEMRL/Stock-Trading-Environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBdCAmkUhs8X",
        "outputId": "811728bd-33dd-4d58-d2b2-8751d571e51a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/UIT/Mạng Neural và Giải thuật di truyền/Project/CEMRL/Stock-Trading-Environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "ES2EqHsNKgbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gym_anytrading\n",
        "from env.StockTradingEnv import StockTradingEnv\n",
        "# del StockTradingEnv"
      ],
      "metadata": {
        "id": "MDqGTS09Mq2f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/UIT/Mạng Neural và Giải thuật di truyền/Project/CEMRL/Stock-Trading-Environment/data/AAPL.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "# df.dtypes\n",
        "# df.set_index('Date', inplace=True)\n",
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "971FGYpTXqmO",
        "outputId": "c8313bba-bae6-42b8-b12b-856cf7a6ccbb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5250</th>\n",
              "      <td>5250</td>\n",
              "      <td>2018-11-12</td>\n",
              "      <td>199.00</td>\n",
              "      <td>199.8500</td>\n",
              "      <td>193.7900</td>\n",
              "      <td>194.17</td>\n",
              "      <td>51135518.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5251</th>\n",
              "      <td>5251</td>\n",
              "      <td>2018-11-13</td>\n",
              "      <td>191.63</td>\n",
              "      <td>197.1800</td>\n",
              "      <td>191.4501</td>\n",
              "      <td>192.23</td>\n",
              "      <td>46882936.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5252</th>\n",
              "      <td>5252</td>\n",
              "      <td>2018-11-14</td>\n",
              "      <td>193.90</td>\n",
              "      <td>194.4800</td>\n",
              "      <td>185.9300</td>\n",
              "      <td>186.80</td>\n",
              "      <td>60800957.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5253</th>\n",
              "      <td>5253</td>\n",
              "      <td>2018-11-15</td>\n",
              "      <td>188.39</td>\n",
              "      <td>191.9700</td>\n",
              "      <td>186.9000</td>\n",
              "      <td>191.41</td>\n",
              "      <td>46478801.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5254</th>\n",
              "      <td>5254</td>\n",
              "      <td>2018-11-16</td>\n",
              "      <td>190.50</td>\n",
              "      <td>194.9695</td>\n",
              "      <td>189.4600</td>\n",
              "      <td>193.53</td>\n",
              "      <td>36186440.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0       Date    Open      High       Low   Close      Volume\n",
              "5250        5250 2018-11-12  199.00  199.8500  193.7900  194.17  51135518.0\n",
              "5251        5251 2018-11-13  191.63  197.1800  191.4501  192.23  46882936.0\n",
              "5252        5252 2018-11-14  193.90  194.4800  185.9300  186.80  60800957.0\n",
              "5253        5253 2018-11-15  188.39  191.9700  186.9000  191.41  46478801.0\n",
              "5254        5254 2018-11-16  190.50  194.9695  189.4600  193.53  36186440.0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = StockTradingEnv(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4dyTIYUY_to",
        "outputId": "a9a7e8ed-bb81-48ca-c2df-78ba63bf844d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = int(env.action_space.high[0])\n"
      ],
      "metadata": {
        "id": "Cbgn1lgHZFQt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = Memory(args.mem_size, state_dim, action_dim)"
      ],
      "metadata": {
        "id": "53T1ASrBiXQ5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic = Critic(state_dim, action_dim, max_action, args)\n",
        "critic_t = Critic(state_dim, action_dim, max_action, args)"
      ],
      "metadata": {
        "id": "flUW2T7djF-v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor = Actor(state_dim, action_dim, max_action, args)\n",
        "actor_t = Actor(state_dim, action_dim, max_action, args)"
      ],
      "metadata": {
        "id": "GWI--cQHnjtz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_noise = GaussianNoise(action_dim, sigma=args.gauss_sigma)"
      ],
      "metadata": {
        "id": "t-6eMTPWqgYx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_CUDA:\n",
        "    print(\"Use Cuda\")\n",
        "    critic.cuda()\n",
        "    critic_t.cuda()\n",
        "    actor.cuda()\n",
        "    actor_t.cuda()\n",
        "\n",
        "# CEM\n",
        "es = sepCEM(actor.get_size(), \n",
        "            mu_init=actor.get_params(), \n",
        "            sigma_init=args.sigma_init, \n",
        "            damp=args.damp, \n",
        "            damp_limit=args.damp_limit, \n",
        "            pop_size=args.pop_size, \n",
        "            antithetic=not args.pop_size % 2, parents=args.pop_size // 2, elitism=args.elitism)"
      ],
      "metadata": {
        "id": "iTauy5JirU6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620eb717-4a6c-468e-9f2a-fd8f5ab580f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use Cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_cpt = 0\n",
        "total_steps = 0\n",
        "actor_steps = 0\n",
        "df = pd.DataFrame(columns=[\"total_steps\", \"average_score\",\n",
        "                            \"average_score_rl\", \"average_score_ea\", \"best_score\"])"
      ],
      "metadata": {
        "id": "XwIljHZ3rIwv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while total_steps < args.max_steps:\n",
        "\n",
        "    fitness = []\n",
        "    fitness_ = []\n",
        "    es_params = es.ask(args.pop_size)\n",
        "\n",
        "    # udpate the rl actors and the critic\n",
        "    if total_steps > args.start_steps:\n",
        "\n",
        "        for i in range(args.n_grad):\n",
        "\n",
        "            # set params\n",
        "            actor.set_params(es_params[i])\n",
        "            actor_t.set_params(es_params[i])\n",
        "            actor.optimizer = torch.optim.Adam(\n",
        "                actor.parameters(), lr=args.actor_lr)\n",
        "\n",
        "            # critic update\n",
        "            for _ in tqdm(range(actor_steps // args.n_grad)):\n",
        "                critic.update(memory, args.batch_size, actor, critic_t)\n",
        "\n",
        "            # actor update\n",
        "            for _ in tqdm(range(actor_steps)):\n",
        "                actor.update(memory, args.batch_size,\n",
        "                                critic, actor_t)\n",
        "\n",
        "            # get the params back in the population\n",
        "            es_params[i] = actor.get_params()\n",
        "    actor_steps = 0\n",
        "\n",
        "    # evaluate noisy actor(s)\n",
        "    for i in range(args.n_noisy):\n",
        "        actor.set_params(es_params[i])\n",
        "        f, steps = evaluate(actor, env, memory=memory, n_episodes=args.n_episodes, render=args.render, noise=a_noise)\n",
        "        actor_steps += steps\n",
        "        prCyan('Noisy actor {} fitness:{}'.format(i, f))\n",
        "\n",
        "    # evaluate all actors\n",
        "    for params in es_params:\n",
        "\n",
        "        actor.set_params(params)\n",
        "        f, steps = evaluate(actor, env, memory=memory, n_episodes=args.n_episodes, render=args.render)\n",
        "        actor_steps += steps\n",
        "        fitness.append(f)\n",
        "\n",
        "        # print scores\n",
        "        prLightPurple('Actor fitness:{}'.format(f))\n",
        "\n",
        "    # update es\n",
        "    es.tell(es_params, fitness)\n",
        "\n",
        "    # update step counts\n",
        "    total_steps += actor_steps\n",
        "    step_cpt += actor_steps\n",
        "\n",
        "    # save stuff\n",
        "    if step_cpt >= args.period:\n",
        "\n",
        "        # evaluate mean actor over several runs. Memory is not filled\n",
        "        # and steps are not counted\n",
        "        actor.set_params(es.mu)\n",
        "        f_mu, _ = evaluate(actor, env, memory=None, n_episodes=args.n_eval,\n",
        "                            render=args.render)\n",
        "        prRed('Actor Mu Average Fitness:{}'.format(f_mu))\n",
        "\n",
        "        df.to_pickle(args.output + \"/log.pkl\")\n",
        "        res = {\"total_steps\": total_steps,\n",
        "                \"average_score\": np.mean(fitness),\n",
        "                \"average_score_half\": np.mean(np.partition(fitness, args.pop_size // 2 - 1)[args.pop_size // 2:]),\n",
        "                \"average_score_rl\": np.mean(fitness[:args.n_grad]),\n",
        "                \"average_score_ea\": np.mean(fitness[args.n_grad:]),\n",
        "                \"best_score\": np.max(fitness),\n",
        "                \"mu_score\": f_mu}\n",
        "\n",
        "        if args.save_all_models:\n",
        "            os.makedirs(args.output + \"/{}_steps\".format(total_steps),\n",
        "                        exist_ok=True)\n",
        "            critic.save_model(\n",
        "                args.output + \"/{}_steps\".format(total_steps), \"critic\")\n",
        "            actor.set_params(es.mu)\n",
        "            actor.save_model(\n",
        "                args.output + \"/{}_steps\".format(total_steps), \"actor_mu\")\n",
        "        else:\n",
        "            critic.save_model(args.output, \"critic\")\n",
        "            actor.set_params(es.mu)\n",
        "            actor.save_model(args.output, \"actor\")\n",
        "        df = df.append(res, ignore_index=True)\n",
        "        step_cpt = 0\n",
        "        print(res)\n",
        "\n",
        "    print(\"Total steps\", total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8qaMx4IyZbC",
        "outputId": "472525d2-711e-4eb7-9d72-334421445ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mActor fitness:193299.44425220048\u001b[00m\n",
            "\u001b[94mActor fitness:1105954.6379850004\u001b[00m\n",
            "\u001b[94mActor fitness:3989.8767462166393\u001b[00m\n",
            "\u001b[94mActor fitness:60471.880861721525\u001b[00m\n",
            "\u001b[94mActor fitness:35265.53269394196\u001b[00m\n",
            "\u001b[94mActor fitness:20265.469026603372\u001b[00m\n",
            "\u001b[94mActor fitness:25991.28825629195\u001b[00m\n",
            "\u001b[94mActor fitness:21652.68408475084\u001b[00m\n",
            "\u001b[94mActor fitness:98552.62403664511\u001b[00m\n",
            "\u001b[94mActor fitness:23968.671349353823\u001b[00m\n",
            "[0.00100398 0.00126551 0.00099924 ... 0.00144615 0.00125553 0.00105746]\n",
            "\u001b[91mActor Mu Average Fitness:114770.35930721062\u001b[00m\n",
            "{'total_steps': 47277, 'average_score': 158941.2109292726, 'average_score_half': 298708.82396590186, 'average_score_rl': 279796.2745078161, 'average_score_ea': 38086.14735072902, 'best_score': 1105954.6379850004, 'mu_score': 114770.35930721062}\n",
            "Total steps 47277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9455/9455 [00:39<00:00, 239.58it/s]\n",
            "100%|██████████| 47277/47277 [03:24<00:00, 230.64it/s]\n",
            "100%|██████████| 9455/9455 [00:41<00:00, 230.29it/s]\n",
            "100%|██████████| 47277/47277 [03:14<00:00, 243.21it/s]\n",
            "100%|██████████| 9455/9455 [00:37<00:00, 251.04it/s]\n",
            "100%|██████████| 47277/47277 [03:36<00:00, 218.61it/s]\n",
            "100%|██████████| 9455/9455 [00:38<00:00, 248.36it/s]\n",
            "100%|██████████| 47277/47277 [03:31<00:00, 223.85it/s]\n",
            "100%|██████████| 9455/9455 [00:38<00:00, 242.72it/s]\n",
            "100%|██████████| 47277/47277 [03:15<00:00, 241.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mActor fitness:113.92291497949694\u001b[00m\n",
            "\u001b[94mActor fitness:113.90846661473444\u001b[00m\n",
            "\u001b[94mActor fitness:-1588.3456906219344\u001b[00m\n",
            "\u001b[94mActor fitness:113.95252121159118\u001b[00m\n",
            "\u001b[94mActor fitness:3514.0205413655194\u001b[00m\n",
            "\u001b[94mActor fitness:2107.2112110853545\u001b[00m\n",
            "\u001b[94mActor fitness:35269.168236479214\u001b[00m\n",
            "\u001b[94mActor fitness:41221.71350532527\u001b[00m\n",
            "\u001b[94mActor fitness:14725.009621807369\u001b[00m\n",
            "\u001b[94mActor fitness:464.2987920451782\u001b[00m\n",
            "[0.00108084 0.00107813 0.00131796 ... 0.14634938 0.00110256 0.02896247]\n",
            "\u001b[91mActor Mu Average Fitness:-2188.031492099604\u001b[00m\n",
            "{'total_steps': 57978, 'average_score': 9605.48601202918, 'average_score_half': 19367.424623212548, 'average_score_rl': 453.4917507098815, 'average_score_ea': 18757.48027334848, 'best_score': 41221.71350532527, 'mu_score': -2188.031492099604}\n",
            "Total steps 57978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2140/2140 [00:08<00:00, 247.71it/s]\n",
            "100%|██████████| 10701/10701 [00:42<00:00, 248.95it/s]\n",
            "100%|██████████| 2140/2140 [00:08<00:00, 250.31it/s]\n",
            "100%|██████████| 10701/10701 [01:00<00:00, 177.23it/s]\n",
            "100%|██████████| 2140/2140 [00:08<00:00, 261.93it/s]\n",
            "100%|██████████| 10701/10701 [00:42<00:00, 249.14it/s]\n",
            "100%|██████████| 2140/2140 [00:08<00:00, 255.95it/s]\n",
            "100%|██████████| 10701/10701 [00:40<00:00, 265.81it/s]\n",
            "100%|██████████| 2140/2140 [00:08<00:00, 264.74it/s]\n",
            "100%|██████████| 10701/10701 [00:40<00:00, 265.37it/s]\n",
            "/content/drive/MyDrive/UIT/Mạng Neural và Giải thuật di truyền/Project/CEMRL/Stock-Trading-Environment/env/StockTradingEnv.py:79: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  prev_cost + additional_cost) / (self.shares_held + shares_bought)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mActor fitness:488142.0538782354\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:11350.748723180051\u001b[00m\n",
            "\u001b[94mActor fitness:16612.81565234455\u001b[00m\n",
            "\u001b[94mActor fitness:11629.216032497465\u001b[00m\n",
            "\u001b[94mActor fitness:446.75298373379667\u001b[00m\n",
            "\u001b[94mActor fitness:-1422.4047523166369\u001b[00m\n",
            "[0.00110146 0.00164202 0.001434   ... 0.02504692 0.00117624 0.00734977]\n",
            "\u001b[91mActor Mu Average Fitness:6889312.5\u001b[00m\n",
            "{'total_steps': 102106, 'average_score': 2808400.9182517673, 'average_score_half': 5609078.4107756475, 'average_score_rl': 5609078.4107756475, 'average_score_ea': 7723.425727887845, 'best_score': 6889312.5, 'mu_score': 6889312.5}\n",
            "Total steps 102106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8825/8825 [00:32<00:00, 267.85it/s]\n",
            "100%|██████████| 44128/44128 [02:46<00:00, 264.93it/s]\n",
            "100%|██████████| 8825/8825 [00:34<00:00, 258.75it/s]\n",
            "100%|██████████| 44128/44128 [02:52<00:00, 255.52it/s]\n",
            "100%|██████████| 8825/8825 [00:33<00:00, 259.71it/s]\n",
            "100%|██████████| 44128/44128 [02:51<00:00, 257.36it/s]\n",
            "100%|██████████| 8825/8825 [00:33<00:00, 266.68it/s]\n",
            "100%|██████████| 44128/44128 [02:48<00:00, 262.42it/s]\n",
            "100%|██████████| 8825/8825 [00:34<00:00, 252.58it/s]\n",
            "100%|██████████| 44128/44128 [02:45<00:00, 266.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:551.2746755010025\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "\u001b[94mActor fitness:6889312.5\u001b[00m\n",
            "[0.00880868 0.0076386  0.00759963 ... 0.01138739 0.00105706 0.00104953]\n",
            "\u001b[91mActor Mu Average Fitness:6889312.5\u001b[00m\n",
            "{'total_steps': 149618, 'average_score': 6200436.37746755, 'average_score_half': 6889312.5, 'average_score_rl': 5511560.2549351, 'average_score_ea': 6889312.5, 'best_score': 6889312.5, 'mu_score': 6889312.5}\n",
            "Total steps 149618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9502/9502 [00:35<00:00, 268.28it/s]\n",
            "100%|██████████| 47512/47512 [02:55<00:00, 269.96it/s]\n",
            "100%|██████████| 9502/9502 [00:35<00:00, 270.27it/s]\n",
            " 13%|█▎        | 6383/47512 [00:23<02:31, 272.05it/s]"
          ]
        }
      ]
    }
  ]
}